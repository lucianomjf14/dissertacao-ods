#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
An√°lise Comparativa: Palavras-Chave Originais vs Normalizadas e Traduzidas
Compara as duas colunas de palavras-chave para identificar diferen√ßas e padr√µes
"""

import pandas as pd
import json
from collections import Counter

def comparar_palavras_chave():
    """
    Compara√ß√£o detalhada entre as duas colunas de palavras-chave
    """
    print("üîç AN√ÅLISE COMPARATIVA: PALAVRAS-CHAVE ORIGINAIS vs NORMALIZADAS")
    print("=" * 80)
    
    # Carregar dados das an√°lises anteriores
    try:
        with open('analise_palavras_chave.json', 'r', encoding='utf-8') as f:
            dados_originais = json.load(f)
        
        with open('analise_palavras_chave_normalizadas.json', 'r', encoding='utf-8') as f:
            dados_normalizados = json.load(f)
        
        print("‚úÖ Dados carregados com sucesso!")
    except FileNotFoundError as e:
        print(f"‚ùå Erro: Arquivo n√£o encontrado - {e}")
        return
    
    # Estat√≠sticas comparativas
    print(f"\nüìä COMPARA√á√ÉO ESTAT√çSTICA:")
    print(f"{'M√©trica':<40} {'Original':<15} {'Normalizada':<15} {'Diferen√ßa':<15}")
    print("-" * 85)
    
    total_palavras_orig = dados_originais['analise_termos']['total_palavras']
    total_palavras_norm = dados_normalizados['analise_termos']['total_palavras']
    diff_total = total_palavras_norm - total_palavras_orig
    
    termos_unicos_orig = dados_originais['analise_termos']['termos_unicos']
    termos_unicos_norm = dados_normalizados['analise_termos']['termos_unicos']
    diff_unicos = termos_unicos_norm - termos_unicos_orig
    
    media_orig = dados_originais['analise_termos']['media_palavras_por_registro']
    media_norm = dados_normalizados['analise_termos']['media_palavras_por_registro']
    diff_media = media_norm - media_orig
    
    print(f"{'Total de palavras':<40} {total_palavras_orig:<15} {total_palavras_norm:<15} {diff_total:+d}")
    print(f"{'Termos √∫nicos':<40} {termos_unicos_orig:<15} {termos_unicos_norm:<15} {diff_unicos:+d}")
    print(f"{'M√©dia por registro':<40} {media_orig:<15.1f} {media_norm:<15.1f} {diff_media:+.1f}")
    
    # An√°lise de efici√™ncia da normaliza√ß√£o
    print(f"\nüéØ EFICI√äNCIA DA NORMALIZA√á√ÉO:")
    reducao_percentual = (abs(diff_unicos) / termos_unicos_orig) * 100
    print(f"‚Ä¢ Redu√ß√£o de termos √∫nicos: {abs(diff_unicos)} termos ({reducao_percentual:.1f}%)")
    print(f"‚Ä¢ Consolida√ß√£o de vocabul√°rio: {'‚úÖ Eficaz' if diff_unicos < 0 else '‚ùå Ineficaz'}")
    
    # Carregar listas completas de termos
    df_orig = pd.read_csv('termos_palavras_chave.csv')
    df_norm = pd.read_csv('termos_palavras_chave_normalizadas.csv')
    
    termos_orig_set = set(df_orig['termo'].str.lower())
    termos_norm_set = set(df_norm['termo'].str.lower())
    
    # An√°lise de sobreposi√ß√£o
    termos_comuns = termos_orig_set.intersection(termos_norm_set)
    termos_apenas_orig = termos_orig_set - termos_norm_set
    termos_apenas_norm = termos_norm_set - termos_orig_set
    
    print(f"\nüîÑ AN√ÅLISE DE SOBREPOSI√á√ÉO:")
    print(f"‚Ä¢ Termos comuns: {len(termos_comuns)}")
    print(f"‚Ä¢ Apenas na original: {len(termos_apenas_orig)}")
    print(f"‚Ä¢ Apenas na normalizada: {len(termos_apenas_norm)}")
    print(f"‚Ä¢ Taxa de sobreposi√ß√£o: {(len(termos_comuns)/max(len(termos_orig_set), len(termos_norm_set)))*100:.1f}%")
    
    # Top 10 termos mais frequentes - compara√ß√£o
    print(f"\nüèÜ COMPARA√á√ÉO DOS TOP 10 TERMOS:")
    print(f"{'Posi√ß√£o':<10} {'Original':<35} {'Freq':<8} {'Normalizada':<35} {'Freq':<8}")
    print("-" * 96)
    
    top_orig = list(dados_originais['top_20_termos'].items())[:10]
    top_norm = list(dados_normalizados['top_20_termos'].items())[:10]
    
    for i in range(10):
        pos = i + 1
        termo_orig, freq_orig = top_orig[i] if i < len(top_orig) else ("", 0)
        termo_norm, freq_norm = top_norm[i] if i < len(top_norm) else ("", 0)
        print(f"{pos:<10} {termo_orig:<35} {freq_orig:<8} {termo_norm:<35} {freq_norm:<8}")
    
    # An√°lise de tradu√ß√µes/normaliza√ß√µes mais significativas
    print(f"\nüîÑ EXEMPLOS DE NORMALIZA√á√ÉO/TRADU√á√ÉO:")
    
    # Mapeamento manual de alguns termos conhecidos
    mapeamentos_esperados = {
        'smart cities': 'cidades inteligentes',
        'smart city': 'cidades inteligentes',
        'sustainable development': 'desenvolvimento sustent√°vel',
        'artificial intelligence': 'intelig√™ncia artificial',
        'internet of things': 'iot',
        'digital twins': 'g√™meos digitais',
        'climate change': 'mudan√ßas clim√°ticas',
        'sustainable cities': 'cidades sustent√°veis'
    }
    
    print("‚Ä¢ Principais tradu√ß√µes identificadas:")
    for orig, norm in mapeamentos_esperados.items():
        if orig in df_orig['termo'].str.lower().values and norm in df_norm['termo'].str.lower().values:
            freq_orig = df_orig[df_orig['termo'].str.lower() == orig]['frequencia'].iloc[0] if len(df_orig[df_orig['termo'].str.lower() == orig]) > 0 else 0
            freq_norm = df_norm[df_norm['termo'].str.lower() == norm]['frequencia'].iloc[0] if len(df_norm[df_norm['termo'].str.lower() == norm]) > 0 else 0
            print(f"  - '{orig}' ({freq_orig}x) ‚Üí '{norm}' ({freq_norm}x)")
    
    # An√°lise de idioma
    print(f"\nüåê AN√ÅLISE DE IDIOMA:")
    
    # Contar termos em ingl√™s vs portugu√™s (heur√≠stica)
    termos_ingles_orig = sum(1 for termo in df_orig['termo'] if any(palavra in termo.lower() for palavra in ['smart', 'city', 'cities', 'sustainable', 'development', 'internet', 'things', 'intelligence', 'data', 'digital', 'climate', 'change']))
    termos_ingles_norm = sum(1 for termo in df_norm['termo'] if any(palavra in termo.lower() for palavra in ['smart', 'city', 'cities', 'sustainable', 'development', 'internet', 'things', 'intelligence', 'data', 'digital', 'climate', 'change']))
    
    print(f"‚Ä¢ Termos em ingl√™s (estimado):")
    print(f"  - Original: {termos_ingles_orig} ({(termos_ingles_orig/termos_unicos_orig)*100:.1f}%)")
    print(f"  - Normalizada: {termos_ingles_norm} ({(termos_ingles_norm/termos_unicos_norm)*100:.1f}%)")
    
    # Salvar relat√≥rio comparativo
    relatorio = {
        'resumo_comparativo': {
            'total_palavras': {
                'original': total_palavras_orig,
                'normalizada': total_palavras_norm,
                'diferenca': diff_total
            },
            'termos_unicos': {
                'original': termos_unicos_orig,
                'normalizada': termos_unicos_norm,
                'diferenca': diff_unicos,
                'reducao_percentual': round(reducao_percentual, 1)
            },
            'sobreposicao': {
                'termos_comuns': len(termos_comuns),
                'apenas_original': len(termos_apenas_orig),
                'apenas_normalizada': len(termos_apenas_norm),
                'taxa_sobreposicao': round((len(termos_comuns)/max(len(termos_orig_set), len(termos_norm_set)))*100, 1)
            }
        },
        'eficiencia_normalizacao': {
            'consolidacao_eficaz': diff_unicos < 0,
            'termos_eliminados': abs(diff_unicos) if diff_unicos < 0 else 0,
            'avaliacao': 'Eficaz' if diff_unicos < 0 else 'Ineficaz'
        }
    }
    
    with open('relatorio_comparativo_palavras_chave.json', 'w', encoding='utf-8') as f:
        json.dump(relatorio, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ Relat√≥rio comparativo salvo em 'relatorio_comparativo_palavras_chave.json'")
    
    return relatorio

if __name__ == "__main__":
    relatorio = comparar_palavras_chave()
